
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import GBTClassifier
from pyspark.sql.functions import col
 
spark = SparkSession.builder.enableHiveSupport().getOrCreate()
spark.conf.set("spark.sql.sources.default", "hive")
spark.sql(f"USE sb_gf")
 

#DefiniciÃ³n variables a aplicar al modelo
variables_diccionario={
    # 1 #
    'edad':{'Tipo':'Discretizada','J':[0,3,5,15]},
    # 2 #
    'cap_ahorro_total_actual':{'Tipo':'Discretizada','J':[-10000,0,50000]},
    # 3 #
    'ingresos_efectivo_actual':{'Tipo':'Discretizada','J':[6000]},
    # 4 #
    'var_gastos_consumo_mes_actual_trim_anterior':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1]},
    # 5 #
    'ingresos_financieros_promedio_trim_anterior':{'Tipo':'Discretizada','J':[30000]},
    # 6 #
    'ingresos_duros_actual':{'Tipo':'Discretizada','J':[100000]},
    # 7 #
    'vinc_saldos_nuevo':{'Tipo':'Discretizada','J':[11]},
    # 8 #
    'var_12m':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1,2]},
    # 9 #
    'variacion_saldo_semestral_num':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1,2]},
    # 10 #
    'var_gastos_transferencia_mes_actual_trim_anterior':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1]},
    # 11 #
    'contador_sube':{'Tipo':'Discretizada','J':[2,4,6,8]},
    # 12 #
    'saldo_medio_actual':{'Tipo':'Discretizada','J':[15000,50000,300000]},
    # 13 #
    'var_gastos_financieros_trim_actual_anterior':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1]},
    # 14 #
    'var_cap_ahorro_dura_mes_actual_trim_anterior':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1]},
    # 15 #
    'ingreso_total_actual':{'Tipo':'Discretizada','J':[30000,300000,1500000]},
    # 16 #
    'gasto_total_actual':{'Tipo':'Discretizada','J':[-1500000,-300000,-30000]},
    # 17 #
    'variacion_saldo_trimestral_num':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1,2]},
    # 18 #
    'variacion_saldo_12m_num':{'Tipo':'Discretizada','J':[-1,-.5,-.2,0,.2,.5,1,2]}
}

lista_variables=['nuevo_nivel_mod']
for variable in variables_diccionario:
    if variables_diccionario[variable]['Tipo']=='Discretizada':
        lista_variables.append(f'{variable}_buckets')
    else:
        lista_variables.append(f'{variable}')

print(f'LISTA VARIABLES: {lista_variables}')

df_00_total_index=spark.sql('''SELECT * FROM sb_gf.ml_ready_reentrenamiento_simplai_2024_personas_juridicas_modelo_indexed_cortes_expertos_19_variables''')


#### PREPARACION TRAIN TEST ####
df_00 = df_00_total_index.filter(
    (col("fec_info") >= "202203") & (col("fec_info") <= "202210") &
    ((col("riesgo_actual").isNull()) | (col("riesgo_actual") < 150000000)) &
    ((col("saldo_medio_actual").isNull()) | (col("saldo_medio_actual") < 150000000)) &
    ((col("ingreso_total_actual").isNull()) | (col("ingreso_total_actual") < 150000000)) &
    ((col("gasto_total_actual").isNull()) | (col("gasto_total_actual") > -150000000))
)

df_01=df_00.na.fill(value = 0, subset = lista_variables)
assemble_00 = VectorAssembler(inputCols = lista_variables, outputCol = 'features')
df_02 = assemble_00.transform(df_01)

#Separacion Train /Test para modelo
df_train, df_test = df_02.randomSplit([0.80, 0.20], seed = 2)
 
#Entrenamiento del modelo
dict_importancia = [{"id": index, "feat_name": feat, "fi": -1} for index, feat in enumerate(lista_variables)]
gbt_algorithm = GBTClassifier(featuresCol = 'features', labelCol = 'target')
gbt_model_00 = gbt_algorithm.fit(df_train)
 
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
fs.delete(spark._jvm.org.apache.hadoop.fs.Path("/pro/workspace/gf/models/classification_pj_2024_cortes_expertos_19_variables"), True)
gbt_model_00.save("/pro/workspace/gf/models/classification_pj_2024_cortes_expertos_19_variables")
 
